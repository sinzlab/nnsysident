{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datajoint as dj\n",
    "dj.config['database.host'] = os.environ['DJ_HOST']\n",
    "dj.config['database.user'] = os.environ['DJ_USER']\n",
    "dj.config['database.password'] = os.environ['DJ_PASS']\n",
    "dj.config['enable_python_native_blobs'] = True\n",
    "\n",
    "name = 'iclr'\n",
    "dj.config['schema_name'] = f\"konstantin_nnsysident_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nnfabrik\n",
    "from nnfabrik.main import *\n",
    "from nnfabrik import builder\n",
    "from nnfabrik.utility.hypersearch import Bayesian\n",
    "\n",
    "from nnsysident.tables.experiments import *\n",
    "from nnsysident.tables.bayesian import *\n",
    "from nnsysident.datasets.mouse_loaders import static_shared_loaders\n",
    "from nnsysident.datasets.mouse_loaders import static_loaders\n",
    "from nnsysident.datasets.mouse_loaders import static_loader\n",
    "\n",
    "def find_number(text, c):\n",
    "    number_list = re.findall(r'%s(\\d+)' % c, text)\n",
    "    if len(number_list) == 0:\n",
    "        number = None\n",
    "    elif len(number_list) == 1:\n",
    "        number = int(number_list[0])\n",
    "    else:\n",
    "        raise ValueError('More than one number found..') \n",
    "    return number\n",
    "\n",
    "def get_transfer(old_experiment_name):\n",
    "    # prepare the Transfer table in a way that all the info about the transferred model is in the DataFrame. Just pd.merge (on transfer_fn and transfer_hash)\n",
    "    # it then with the model that the transferred model was used for. \n",
    "    transfer = pd.DataFrame(Transfer.fetch())\n",
    "    transfer = pd.concat([transfer, transfer['transfer_config'].apply(pd.Series)], axis = 1).drop('transfer_config', axis = 1)\n",
    "\n",
    "    tm = pd.DataFrame((TrainedModel * Dataset * Seed * Experiments.Restrictions & 'experiment_name = \"{}\"'.format(old_experiment_name)).fetch()).rename(\n",
    "        columns = {'model_hash': 't_model_hash', 'trainer_hash': 't_trainer_hash', 'dataset_hash': 't_dataset_hash'})\n",
    "    tm = tm.sort_values('score', ascending=False).drop_duplicates(['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "\n",
    "    transfer = pd.merge(transfer, tm, how='inner', on=['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "    transfer = pd.concat([transfer, transfer['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "    transfer.columns = ['t_' + col if col[:2] != 't_' and col[:8] != 'transfer'  else col for col in transfer.columns]\n",
    "    transfer = transfer.sort_values(['t_multi_match_n', 't_image_n', 't_multi_match_base_seed', 't_image_base_seed'])\n",
    "    return transfer\n",
    "\n",
    "# def get_transfer(transfer_hashes):\n",
    "#     # prepare the Transfer table in a way that all the info about the transferred model is in the DataFrame. Just pd.merge (on transfer_fn and transfer_hash)\n",
    "#     # it then with the model that the transferred model was used for. \n",
    "    \n",
    "#     transfer = pd.DataFrame((Transfer & 'transfer_hash in {}'.format(tuple(transfer_hashes))).fetch())\n",
    "#     transfer = pd.concat([transfer, transfer['transfer_config'].apply(pd.Series)], axis = 1).drop('transfer_config', axis = 1)\n",
    "\n",
    "#     restriction = transfer.rename(columns = {'t_model_hash': 'model_hash', 't_dataset_hash': 'dataset_hash', 't_trainer_hash': 'trainer_hash'})            \n",
    "#     restriction = restriction[['model_hash', 'dataset_hash', 'trainer_hash']].to_dict('records')\n",
    "\n",
    "#     tm = pd.DataFrame((TrainedModel * Dataset * Seed & restriction).fetch()).rename(\n",
    "#         columns = {'model_hash': 't_model_hash', 'trainer_hash': 't_trainer_hash', 'dataset_hash': 't_dataset_hash'})               \n",
    "#     tm = tm.sort_values('score', ascending=False).drop_duplicates(['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "\n",
    "#     transfer = pd.merge(transfer, tm, how='inner', on=['t_model_hash', 't_trainer_hash', 't_dataset_hash'])\n",
    "#     transfer = pd.concat([transfer, transfer['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "#     transfer.columns = ['t_' + col if col[:2] != 't_' and col[:8] != 'transfer'  else col for col in transfer.columns]\n",
    "#     transfer = transfer.sort_values(['t_multi_match_n', 't_image_n', 't_multi_match_base_seed', 't_image_base_seed'])\n",
    "#     return transfer\n",
    "\n",
    "\n",
    "def get_transfer_entries(old_experiment_name, overall_best):\n",
    "    tm = pd.DataFrame((TrainedModel * Dataset * Seed * Experiments.Restrictions & 'experiment_name=\"{}\"'.format(old_experiment_name)).fetch())\n",
    "    tm = pd.concat([tm, tm['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "\n",
    "    model_fn = np.unique(tm['model_fn'])\n",
    "    assert len(model_fn) == 1 ,\"Must have exactly 1 model function in experiment\"\n",
    "    model_fn = model_fn[0] \n",
    "\n",
    "    # Filter out best model(s) \n",
    "    if overall_best is True:\n",
    "        tm = tm.loc[(tm['multi_match_n'] == tm['multi_match_n'].max()) & (tm['image_n'] == tm['image_n'].max())]\n",
    "    else:\n",
    "        tm = tm.loc[(tm['multi_match_n'] == tm['multi_match_n'].max())]\n",
    "    tm = tm.sort_values('score', ascending=False).drop_duplicates(['multi_match_n', 'image_n', 'multi_match_base_seed', 'image_base_seed']).sort_values(['multi_match_n', 'image_n'])\n",
    "\n",
    "    # make entries for Trasfer table\n",
    "    entries = [dict(transfer_fn='nnsysident.models.transfer_functions.core_transfer', \n",
    "                     transfer_config = dict(t_model_hash=row.model_hash, t_dataset_hash=row.dataset_hash, t_trainer_hash=row.trainer_hash),\n",
    "                     transfer_comment=model_fn.split('.')[-1] + ', multi_match_n={}, multi_match_base_seed={}, image_n={}, image_base_seed={}'.format(row.multi_match_n, \n",
    "                                                                                                                                    row.multi_match_base_seed, \n",
    "                                                                                                                                    row.image_n, \n",
    "                                                                                                                                    row.image_base_seed),\n",
    "                     transfer_fabrikant='kklurz') for loc, row in tm.iterrows()]\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (direkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dataset entries for different neuron and image seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best hyperparameters\n",
    "\n",
    "data = pd.DataFrame((TrainedModelBayesian() * \n",
    "                     ModelBayesian * \n",
    "                     DatasetBayesian * \n",
    "                     Trainer & \n",
    "                     'model_fn = \"nnsysident.models.models.se2d_fullSXF\"'  \n",
    "                     ).fetch()) #'trainer_hash = \"3c6008284286683e7ce19e9e1269f507\"'\n",
    "data = pd.concat([data, data['dataset_config'].apply(pd.Series)], axis = 1)#.drop('dataset_config', axis = 1)\n",
    "data = pd.concat([data, data['model_config'].apply(pd.Series)], axis = 1)#.drop('model_config', axis = 1)\n",
    "data = pd.concat([data, data['trainer_config'].apply(pd.Series)], axis = 1)#.drop('model_config', axis = 1)\n",
    "\n",
    "import hiplot as hip\n",
    "from nnsysident.tables.bayesian import *\n",
    "\n",
    "one_exp_h = data.set_index('score', drop=False).copy()\n",
    "cols = ['score', 'gamma_readout', 'share_features', 'multi_match_n', 'image_n']\n",
    "hip.Experiment.from_dataframe(one_exp_h[cols]).display(force_full_width=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries from transfer hypersearch\n",
    "\n",
    "gamma_readout = 31.207\n",
    "\n",
    "neuron_n = 5335\n",
    "image_n = 50\n",
    "\n",
    "\n",
    "share_features = True\n",
    "share_transform = False\n",
    "\n",
    "\n",
    "model_comment = 'se2d_fullgaussian2d, neuron_n={}, image_n={}'.format(neuron_n, image_n)\n",
    "model_config = { 'init_mu_range': 0.55,\n",
    "                 'init_sigma': 0.4,\n",
    "                 'input_kern': 15,\n",
    "                 'hidden_kern': 13,\n",
    "                 'gamma_input': 1.0,\n",
    "                 'grid_mean_predictor': {'type': 'cortex',\n",
    "                  'input_dimensions': 2,\n",
    "                  'hidden_layers': 0,\n",
    "                  'hidden_features': 0,\n",
    "                  'final_tanh': False},\n",
    "                \n",
    "                 'gamma_readout': gamma_readout,\n",
    "                 'share_features': share_features,\n",
    "                 'share_transform': share_transform}\n",
    "entry = dict(model_fn = 'nnsysident.models.models.se2d_fullgaussian2d', model_config = model_config, model_fabrikant = 'kklurz', model_comment = model_comment)\n",
    "Model().add_entry(**entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries from direct hypersearch\n",
    "\n",
    "neuron_n = 50\n",
    "image_n = 50\n",
    "for neuron_n in [50, 500, 3625]:\n",
    "    for image_n in [50, 200, 500, 1000, 2500, 4399]:\n",
    "        share_features = True\n",
    "        share_transform = False\n",
    "        dat = data.loc[(data['multi_match_n'] == neuron_n) & (data['image_n'] == image_n) & (data['share_features'] == True) & (data['share_transform'] == False)]\n",
    "        gamma_readout = dat.loc[dat['score'] == dat['score'].max()].gamma_readout.values[0]\n",
    "        if neuron_n == 3625:\n",
    "            model_comment = 'se2d_fullgaussian2d, neuron_n={}, image_n={}'.format(3597, image_n)\n",
    "        else:\n",
    "            model_comment = 'se2d_fullgaussian2d, neuron_n={}, image_n={}'.format(neuron_n, image_n)\n",
    "        model_config = { 'init_mu_range': 0.55,\n",
    "                         'init_sigma': 0.4,\n",
    "                         'input_kern': 15,\n",
    "                         'hidden_kern': 13,\n",
    "                         'gamma_input': 1.0,\n",
    "                         'grid_mean_predictor': {'type': 'cortex',\n",
    "                          'input_dimensions': 2,\n",
    "                          'hidden_layers': 0,\n",
    "                          'hidden_features': 0,\n",
    "                          'final_tanh': False},\n",
    "\n",
    "                         'gamma_readout': gamma_readout,\n",
    "                         'share_features': share_features,\n",
    "                         'share_transform': share_transform}\n",
    "        entry = dict(model_fn = 'nnsysident.models.models.se2d_fullgaussian2d', model_config = model_config, model_fabrikant = 'kklurz', model_comment = model_comment)\n",
    "        Model().add_entry(**entry, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries for dataset\n",
    "\n",
    "for neuron_n in [50, 500, 1000, 3597]:\n",
    "    for image_n in [50, 200, 500, 1000, 2500, 4399]:\n",
    "        \n",
    "        dataset_comment = 'neuron_n={}, image_n={}'.format(neuron_n, image_n)\n",
    "        dataset_config = {'paths': ['data/static22564-2-12-preproc0.zip',\n",
    "                          'data/static22564-2-13-preproc0.zip',\n",
    "                          'data/static22564-3-8-preproc0.zip',\n",
    "                          'data/static22564-3-12-preproc0.zip'],\n",
    "                         'batch_size': 64,\n",
    "                         'multi_match_n': neuron_n,\n",
    "                         'multi_match_base_seed': 1,\n",
    "                         'image_n': image_n,\n",
    "                         'image_base_seed': 1}\n",
    "\n",
    "        entry = dict(dataset_fn = 'nnsysident.datasets.mouse_loaders.static_shared_loaders', \n",
    "                     dataset_config = dataset_config, \n",
    "                     dataset_fabrikant = 'kklurz', \n",
    "                     dataset_comment = dataset_comment)\n",
    "        Dataset().add_entry(**entry, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries for dataset - different seeds\n",
    "\n",
    "multi_match_base_seeds = [1,2,3,4,5]\n",
    "image_base_seeds = [1]\n",
    "\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_shared_loaders'\n",
    "paths = ['data/static22564-2-12-preproc0.zip',\n",
    "                     'data/static22564-2-13-preproc0.zip',\n",
    "                     'data/static22564-3-8-preproc0.zip',\n",
    "                     'data/static22564-3-12-preproc0.zip']\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[(dataset['multi_match_base_seed'] == 1) & (dataset['image_base_seed'] == 1)]\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "#dataset = dataset.loc[dataset['exclude_neuron_n'].isnull()]\n",
    "\n",
    "for loc, row in dataset.iterrows():\n",
    "    for multi_match_base_seed in multi_match_base_seeds:\n",
    "        for image_base_seed in image_base_seeds:\n",
    "            dataset_config = row['dataset_config']\n",
    "            dataset_config.update(multi_match_base_seed=multi_match_base_seed, image_base_seed=image_base_seed)\n",
    "            Dataset().add_entry(dataset_fn=row['dataset_fn'], \n",
    "                                dataset_config=dataset_config, \n",
    "                                dataset_fabrikant=row['dataset_fabrikant'], \n",
    "                                dataset_comment=row['dataset_comment'], skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static22564-2-12-preproc0.zip',\n",
    "         'data/static22564-2-13-preproc0.zip',\n",
    "         'data/static22564-3-8-preproc0.zip',\n",
    "         'data/static22564-3-12-preproc0.zip']\n",
    "\n",
    "experiment_name = 'Real, Direct, {}, 4-set'.format(model_name)\n",
    "experiment_comment = 'Directly trained on real data with {} and static_shared_loaders of the 4-set. Varying number of neurons and images.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "detach_core = False\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_shared_loaders'\n",
    "trainer_fn = 'nnsysident.training.trainers.standard_trainer'\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "\n",
    "dataset = dataset.loc[(dataset['multi_match_base_seed'].isin([1,2,3,4,5])) & (dataset['image_base_seed'] == 1)] # maybe comment here\n",
    "#dataset = dataset.loc[dataset['exclude_neuron_n'].isnull()] # maybe comment here\n",
    "\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [find_number(row.model_comment, arg + '=') for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "model = model.loc[model['share_features'] == True] # maybe comment here\n",
    "model = model.loc[~model['grid_mean_predictor'].isnull()]\n",
    "model.rename(columns={'neuron_n': 'multi_match_n'}, inplace=True)\n",
    "  \n",
    "    \n",
    "trainer = pd.DataFrame((Trainer & 'trainer_fn=\"{}\"'.format(trainer_fn)).fetch())\n",
    "trainer = pd.concat([trainer, trainer['trainer_config'].apply(pd.Series)], axis = 1)\n",
    "trainer = trainer.loc[(trainer['detach_core'] == detach_core)]    \n",
    "assert len(trainer) == 1, 'Too many trainers!'\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"multi_match_n\", \"image_n\"]).sort_values(['multi_match_n', 'image_n'])\n",
    "\n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': trainer['trainer_hash'].values[0],\n",
    "               'trainer_fn': trainer['trainer_fn'].values[0],\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]\n",
    "\n",
    "Experiments.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "Experiments.Restrictions.insert(experiment, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (SameNI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transfer entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_experiment_name = 'Real, Direct, se2d_fullgaussian2d, 4-set'\n",
    "overall_best = False\n",
    "\n",
    "yes, no = [], []\n",
    "for entry in get_transfer_entries(old_experiment_name = old_experiment_name, overall_best=overall_best):\n",
    "    try:\n",
    "        Transfer().add_entry(**entry)\n",
    "        yes.append(entry)\n",
    "    except:\n",
    "        no.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for experiment\n",
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static22564-2-12-preproc0.zip',\n",
    "             'data/static22564-2-13-preproc0.zip',\n",
    "             'data/static22564-3-8-preproc0.zip',\n",
    "             'data/static22564-3-12-preproc0.zip']\n",
    "\n",
    "experiment_name = 'Real, core_transfer (sameNI), {}, 4-set -> 4-set'.format(model_name)\n",
    "experiment_comment = 'Transfer training on real data with {} and static_shared_loaders of the 4-set. Varying number of images in the transfer core.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_shared_loaders'\n",
    "trainer_fn = \"nnsysident.training.trainers.standard_trainer\"\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "\n",
    "trainer = pd.DataFrame((Trainer & 'trainer_fn=\"{}\"'.format(trainer_fn)).fetch())\n",
    "trainer = pd.concat([trainer, trainer['trainer_config'].apply(pd.Series)], axis = 1).drop('trainer_config', axis = 1)\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [find_number(row.model_comment, arg + '=') for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1).drop('model_config', axis = 1)\n",
    "model.rename(columns={'neuron_n': 'multi_match_n'}, inplace=True)\n",
    "\n",
    "# Restrict here\n",
    "model = model.loc[model['multi_match_n'] == 1000]\n",
    "\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "dataset = dataset.loc[(dataset['multi_match_n'] == 1000) & (dataset['image_n'] == 4399)]\n",
    "dataset = dataset.loc[(dataset['multi_match_base_seed'].isin([1,2,3,4,5])) & (dataset['image_base_seed'] == 1)]\n",
    "dataset = dataset.loc[dataset['exclude_multi_match_n'] == 3597]\n",
    "trainer = trainer.loc[trainer['detach_core'] == True]\n",
    "assert len(trainer) == 1, 'Too many trainers!'\n",
    "\n",
    "\n",
    "old_experiment_name = 'Real, Direct, {}, 4-set'.format(model_name)\n",
    "transfer = get_transfer(old_experiment_name)\n",
    "transfer = transfer.rename(columns = {'t_multi_match_base_seed': 'multi_match_base_seed', \n",
    "                                      't_image_base_seed': 'image_base_seed', \n",
    "                                      't_multi_match_n': 'multi_match_n', \n",
    "                                      't_image_n':'image_n'})\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"multi_match_n\", \"image_n\"]).sort_values(['multi_match_n', 'image_n'])\n",
    "combinations = pd.merge(combinations, transfer, on=['multi_match_base_seed',\n",
    "                                                    'image_base_seed'])\n",
    "\n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': trainer['trainer_hash'].values[0],\n",
    "               'trainer_fn': trainer['trainer_fn'].values[0],\n",
    "               'transfer_hash': row['transfer_hash'], \n",
    "               \"transfer_fn\": row['transfer_fn'],\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperimentsTransfer.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "ExperimentsTransfer.Restrictions.insert(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dataset entries for different neuron and image seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(Dataset.fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "dataset = dataset.loc[(dataset['multi_match_base_seed'].isin([1,2,3,4,5])) & (dataset['image_base_seed'] == 1)] # maybe comment here\n",
    "dataset = dataset.loc[(dataset['multi_match_n'] == 3597)] # take any one number of neurons\n",
    "dataset = dataset.loc[dataset['exclude_multi_match_n'].isnull()]\n",
    "\n",
    "entries = []\n",
    "for loc, row in dataset.iterrows():\n",
    "    exclude_multi_match_n = 3597\n",
    "    multi_match_n = 1000\n",
    "    entry = dict(dataset_fn='nnsysident.datasets.mouse_loaders.static_shared_loaders', \n",
    "                 dataset_config = dict(paths=row.paths,\n",
    "                                       batch_size=64,\n",
    "                                       multi_match_n = multi_match_n,\n",
    "                                      multi_match_base_seed = row.multi_match_base_seed,\n",
    "                                      image_n = row.image_n,\n",
    "                                      image_base_seed = row.image_base_seed,\n",
    "                                      exclude_multi_match_n = exclude_multi_match_n),\n",
    "                 dataset_comment='multi_match_n={}, image_n={}, exclude_multi_match_n={}'.format(multi_match_n, row.image_n, exclude_multi_match_n),\n",
    "                 dataset_fabrikant='kklurz')\n",
    "    entries.append(entry)\n",
    "    \n",
    "for entry in entries:\n",
    "    Dataset().add_entry(**entry, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transfer entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_experiment_name = 'Real, Direct, se2d_spatialxfeaturelinear, 4-set'\n",
    "overall_best = True\n",
    "\n",
    "for entry in get_transfer_entries(old_experiment_name = old_experiment_name, overall_best=overall_best):\n",
    "    Transfer().add_entry(**entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static22564-2-12-preproc0.zip',\n",
    "             'data/static22564-2-13-preproc0.zip',\n",
    "             'data/static22564-3-8-preproc0.zip',\n",
    "             'data/static22564-3-12-preproc0.zip']\n",
    "\n",
    "\n",
    "experiment_name = 'Real, core_transfer (best), {}, 4-set -> 4-set'.format(model_name)\n",
    "experiment_comment = 'Transfer training on real data with {} and static_loaders of the 4-set. Varying number of multi_matchs and images.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_shared_loaders'\n",
    "trainer_fn = \"nnsysident.training.trainers.standard_trainer\"\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "\n",
    "trainer = pd.DataFrame((Trainer & 'trainer_fn=\"{}\"'.format(trainer_fn)).fetch())\n",
    "trainer = pd.concat([trainer, trainer['trainer_config'].apply(pd.Series)], axis = 1).drop('trainer_config', axis = 1)\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [find_number(row.model_comment, arg + '=') for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1).drop('model_config', axis = 1)\n",
    "model.rename(columns={'neuron_n': 'multi_match_n'}, inplace=True)\n",
    "\n",
    "\n",
    "# Restrict here\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "dataset = dataset.loc[(dataset['multi_match_base_seed'].isin([1,2,3,4,5])) & (dataset['image_base_seed'] == 1)]\n",
    "dataset = dataset.loc[~ dataset['exclude_multi_match_n'].isnull()]\n",
    "dataset = dataset.loc[dataset['exclude_multi_match_n'] == 3597]\n",
    "trainer = trainer.loc[trainer['detach_core'] == True]\n",
    "assert len(trainer) == 1, 'Too many trainers!'\n",
    "\n",
    "\n",
    "old_experiment_name = 'Real, Direct, {}, 4-set'.format(model_name)\n",
    "transfer = get_transfer(old_experiment_name)\n",
    "transfer = transfer.loc[(transfer['t_multi_match_n'] == 3597) & (transfer['t_image_n'] == 4399)]\n",
    "transfer = transfer.rename(columns = {'t_multi_match_base_seed': 'multi_match_base_seed', 't_image_base_seed': 'image_base_seed'})\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"multi_match_n\", \"image_n\"]).sort_values(['multi_match_n', 'image_n'])\n",
    "combinations = pd.merge(combinations, transfer, on=['multi_match_base_seed', 'image_base_seed'])\n",
    "\n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': trainer['trainer_hash'].values[0],\n",
    "               'trainer_fn': trainer['trainer_fn'].values[0],\n",
    "               'transfer_hash': row['transfer_hash'], \n",
    "               \"transfer_fn\": row['transfer_fn'],\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperimentsTransfer.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "ExperimentsTransfer.Restrictions.insert(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Experiment (animal transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train direct on test animal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static20457-5-9-preproc0.zip']\n",
    "\n",
    "experiment_name = 'Real, Direct, {}, 20457-5-9'.format(model_name)\n",
    "experiment_comment = 'Directly trained on real data with {} and static_loaders of 20457-5-9. Varying number of images.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "\n",
    "dataset = dataset.loc[(dataset['neuron_n'] == 5335)].sort_values('image_n') # maybe comment here\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'multi_match_n', 'image_n']:\n",
    "    model[arg] = [find_number(row.model_comment, arg + '=') for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "model = model.loc[model['multi_match_n'].isnull()] # maybe comment this\n",
    "model = model.loc[model['neuron_n'] == 5335] # maybe comment this\n",
    "model = model.loc[~model['grid_mean_predictor'].isnull()]\n",
    "\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"neuron_n\", \"image_n\"]).sort_values(['neuron_n', 'image_n'])\n",
    "   \n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': 'd41d8cd98f00b204e9800998ecf8427e',\n",
    "               'trainer_fn': 'nnsysident.training.trainers.standard_trainer',\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]\n",
    "\n",
    "#Experiments.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "#Experiments.Restrictions.insert(experiment, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transfer entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add respective experiment (restriction) entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'se2d_fullgaussian2d'\n",
    "paths = ['data/static20457-5-9-preproc0.zip']\n",
    "\n",
    "experiment_name = 'Real, core_transfer (animal), {}, 11-set -> 20457-5-9'.format(model_name)\n",
    "experiment_comment = 'Transfer training from the 11-set to 20457-5-9 with {} and static_loaders. Varying number of images.'.format(model_name)\n",
    "fabrikant_name = 'kklurz'\n",
    "\n",
    "model_fn = \"nnsysident.models.models.{}\".format(model_name)\n",
    "dataset_fn = 'nnsysident.datasets.mouse_loaders.static_loaders'\n",
    "trainer_fn = \"nnsysident.training.trainers.standard_trainer\"\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame((Dataset & 'dataset_fn = \"{}\"'.format(dataset_fn)).fetch())\n",
    "dataset = pd.concat([dataset, dataset['dataset_config'].apply(pd.Series)], axis = 1)\n",
    "dataset = dataset.loc[[np.isin(row['paths'], [paths])[0] for loc, row in dataset.iterrows()]]\n",
    "dataset = dataset.loc[(dataset['neuron_n'] == 5335)].sort_values('image_n') # maybe comment here\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'multi_match_n', 'image_n']:\n",
    "    model[arg] = [find_number(row.model_comment, arg + '=') for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1)\n",
    "model = model.loc[model['multi_match_n'].isnull()] # maybe comment this\n",
    "model = model.loc[model['neuron_n'] == 5335] # maybe comment this\n",
    "model = model.loc[~model['grid_mean_predictor'].isnull()]\n",
    "\n",
    "trainer = pd.DataFrame((Trainer & 'trainer_fn=\"{}\"'.format(trainer_fn)).fetch())\n",
    "trainer = pd.concat([trainer, trainer['trainer_config'].apply(pd.Series)], axis = 1).drop('trainer_config', axis = 1)\n",
    "trainer = trainer.loc[trainer['detach_core'] == True]\n",
    "\n",
    "transfer_comment = '11 set best core'\n",
    "transfer = pd.DataFrame((Transfer & 'transfer_comment = \"{}\"'.format(transfer_comment)).fetch())\n",
    "\n",
    "combinations = pd.merge(dataset, model, on=[\"neuron_n\", \"image_n\"]).sort_values(['neuron_n', 'image_n'])\n",
    "\n",
    "experiment = [{'dataset_hash': row['dataset_hash'], \n",
    "               'dataset_fn': row['dataset_fn'],\n",
    "               'model_hash': row['model_hash'],\n",
    "               'model_fn': row['model_fn'],\n",
    "               'trainer_hash': trainer['trainer_hash'].values[0],\n",
    "               'trainer_fn': trainer['trainer_fn'].values[0],\n",
    "               'transfer_hash': transfer['transfer_hash'].values[0], \n",
    "               \"transfer_fn\": transfer['transfer_fn'].values[0],\n",
    "                 'experiment_name': experiment_name} for loc, row in combinations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExperimentsTransfer.insert1(dict(experiment_name=experiment_name, experiment_fabrikant=fabrikant_name, experiment_comment=experiment_comment))\n",
    "#ExperimentsTransfer.Restrictions.insert(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add single entries in main tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = dict(dataset_fn='nnsysident.datasets.mouse_loaders.static_loaders', \n",
    "             dataset_config = dict(paths=paths,\n",
    "                                   batch_size=64,\n",
    "                                   seed=1),\n",
    "             dataset_comment='full dataset',\n",
    "             dataset_fabrikant='kklurz')\n",
    "#Dataset().add_entry(**entry)\n",
    "\n",
    "entry = dict(model_fn='nnsysident.models.models.se2d_fullgaussian2d', \n",
    "             model_config = dict(),\n",
    "             model_comment='default model',\n",
    "             model_fabrikant='kklurz')\n",
    "#Model().add_entry(**entry)\n",
    "\n",
    "entry = dict(trainer_fn='nnsysident.training.trainers.standard_trainer', \n",
    "             trainer_config = dict(detach_core=True),\n",
    "             trainer_comment='default trainer',\n",
    "             trainer_fabrikant='kklurz')\n",
    "#Trainer().add_entry(**entry)\n",
    "\n",
    "entry = dict(transfer_fn='nnsysident.models.transfer_functions.core_transfer', \n",
    "             transfer_config = dict(t_model_hash=\"d41d8cd98f00b204e9800998ecf8427e\", t_dataset_hash='6fa162a20053a013ab4bd31a21950d35', t_trainer_hash='d41d8cd98f00b204e9800998ecf8427e'),\n",
    "             transfer_comment='test transfer',\n",
    "             transfer_fabrikant='kklurz')\n",
    "#Transfer().add_entry(**entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model state dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(state_dict_1, state_dict_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(state_dict_1.items(), state_dict_2.items()):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if (key_item_1[0] == key_item_2[0]):\n",
    "                print('Mismtach found at', key_item_1[0])\n",
    "            else:\n",
    "                raise Exception\n",
    "    if models_differ == 0:\n",
    "        print('Models match perfectly! :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add failed Bayesian to Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = \"nnsysident.models.models.se2d_pointpooled\"\n",
    "\n",
    "all_info = pd.DataFrame((TrainedModelBayesian * ModelBayesian * DatasetBayesian & 'model_fn = \"{}\"'.format(model_fn)).fetch())\n",
    "all_info = pd.concat([all_info, all_info['dataset_config'].apply(pd.Series)], axis = 1).drop('dataset_config', axis = 1)\n",
    "all_info = pd.concat([all_info, all_info['model_config'].apply(pd.Series)], axis = 1)\n",
    "\n",
    "for neuron_n in [100, 1000]:\n",
    "    for image_n in [50, 100, 200, 500, 1000, 4000]:\n",
    "        one_exp = all_info.loc[(all_info['neuron_n'] == neuron_n) & (all_info['image_n'] == image_n) & (~ all_info['hidden_kern'].isnull())].sort_values('score')\n",
    "        best = one_exp.loc[one_exp['score'] == one_exp['score'].max()]\n",
    "        print(len(one_exp))\n",
    "        print(one_exp['neuron_n'].values[0], one_exp['image_n'].values[0])\n",
    "#         Model().add_entry(model_fn=best['model_fn'].values[0],\n",
    "#                           model_config=best['model_config'].values[0],\n",
    "#                           model_fabrikant='kklurz',\n",
    "#                           model_comment='{}, neuron_n={}, image_n={}'.format(best['model_fn'].values[0].split('.')[-1], \n",
    "#                                                                              best['neuron_n'].values[0], best['image_n'].values[0]), skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = \"nnsysident.models.models.se2d_spatialxfeaturelinear\"\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['neuron_n', 'image_n']:\n",
    "    model[arg] = [int(find_number(row.model_comment, arg + '=')[0]) for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1).drop('model_config', axis = 1).sort_values(['neuron_n', 'image_n'])\n",
    "model = model.loc[~model['multi_match_n'].isnull()]\n",
    "model = model.loc[~model['grid_mean_predictor'].isnull()]\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_exp_h = one_exp.set_index('score', drop=False).copy()\n",
    "cols = ['score', 'gamma_readout', 'gamma_input', 'init_mu_range', 'init_sigma']\n",
    "hip.Experiment.from_dataframe(one_exp_h[cols]).display(force_full_width=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add model configs according to similar # I and # N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = \"nnsysident.models.models.se2d_spatialxfeaturelinear\"\n",
    "\n",
    "model = pd.DataFrame((Model & 'model_fn=\"{}\"'.format(model_fn)).fetch())\n",
    "for arg in ['multi_match_n', 'image_n']:\n",
    "    model[arg] = [find_number(row.model_comment, arg + '=') for loc, row in model.iterrows()]\n",
    "model = pd.concat([model, model['model_config'].apply(pd.Series)], axis = 1)\n",
    "model = model.loc[~model['multi_match_n'].isnull()]\n",
    "model = model.loc[~model['grid_mean_predictor'].isnull()]\n",
    "\n",
    "\n",
    "for multi_match_n in [1000]:\n",
    "    new_multi_match_n = 972\n",
    "    for image_n in [50, 200, 500, 1000, 2500, 4399]:\n",
    "        row = model.loc[(model['multi_match_n'] == multi_match_n) & (model['image_n'] == image_n)]\n",
    "        model_config = row['model_config'].values[0]\n",
    "        model_config['gamma_readout'] = model_config['gamma_readout'] + 0.00000001\n",
    "        entry = dict(model_fn=row['model_fn'].values[0],\n",
    "                     model_config=model_config, \n",
    "                     model_fabrikant='kklurz', \n",
    "                     model_comment= '{}, multi_match_n={}, image_n={}'.format(row['model_fn'].values[0].split('.')[-1], \n",
    "                                                                              new_multi_match_n, row['image_n'].values[0]))\n",
    "        Model().add_entry(**entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
